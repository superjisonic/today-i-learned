# 아파치 쿠두

## 아파치 쿠두(Apache Kudu)란?

- 칼럼형 스토리지 엔진으로, 빠른 분석에 유리하다
- 실시간 분석 수행에 유리
- 아파치 하둡 환경의 데이터 처리 프레임워크 대부분과 호환된다

## Kudu vs HBase

보통 둘을 비교하던데.... 일단 비교하려면 HBase가 무엇인지 알아야할듯 싶다.

### HBase란?

하둡은 MapReduce로 분산 데이터 프로세싱을 배치로 하는데, 전체 데이터셋을 검색하기 때문에 시간이 오래걸린다. 이를 해결하기 위해 ***단일 시간 단위로 데이터를 액세스***하기 위해 HBase가 나오게 되었다.

### 잠깐만, Map Reduce가 뭔데? (ㅠㅠ..)

하둡을 다뤄본 적이 없는 나는 부끄럽게도 맵리듀스가 뭔지도 모른다. 아 그거 js에서 나오는거 아니요?(당당)

*그렇다,, javascript의 map과 reduce를 생각하면 될것이다..*

![https://media.vlpt.us/images/kimdukbae/post/ffe11dec-16ee-46c9-97da-fd847a068776/image.png](https://media.vlpt.us/images/kimdukbae/post/ffe11dec-16ee-46c9-97da-fd847a068776/image.png)

[그림 - map과 reduce를 비유한 예시]

하둡은 일단 **HDFS(Hadoop Distributed File System) 에 데이터를 저장**을 하고 분산처리를 한다

데이터 분산처리시, 속도는 빠르지만 **result set을 취합하고 정리**하는 비용도 만만치 않다. 또한 데이터의 양이 들쭉날쭉하면 어떻게 데이터를 효과적으로 나눠서 분산처리할지 문제도 생긴다.

그래서 나온게 `Map`과 `Reduce`!

- Map : 잘게 쪼개진 (Split단계가 선행됨) 파일을 인풋으로 받아서 key, value로 데이터를 묶어준다. key는 데이터의 순서, value는 그 값을 추출한 정보를 담는다.
- Reduce : map의 key를 중심으로 필터링 및 정렬을 진행해서 처리된 데이터를 다시 묶어준다.

여기까지만 하고 하둡의 자세한 맵 리듀스 개념은 새로 정리글을 써야할 것 같다.

### 그래서, HBase가 뭐하는 앤데요..?

일단 특징들을 나열해보면

- HBase = Hadoop database (의 줄인말)
- 하둡기반 분산데이터베이스, 빅데이터 저장위해 사용
- NoSQL
    - 그래서, 스키마 변경없이 자유롭게 데이터 저장 가능
- HDFS(하둡 분산 파일 시스템)위에서 작동
- Java API로 접근 (+ REST ...)
- HBase에 Apache Phoenix와 같은 SQL레이어를 올리면 jdbc드라이버로 분석작업 가능

그리고, 제일 중요한 개념으로 **컬럼 패밀리**가 있다.

### HBase의 컬럼 패밀리란?

HBase의 기본 단위 = 컬럼

→ 컬럼이 모여서 컬럼 패밀리를 구성

→ 컬럼 패밀리가 모여서 테이블을 구성

[https://docs.google.com/drawings/d/1N-TnXQF19nwFYxkuWq_yUWSQ77IH147hZ0d5gDcgXr6/pub?w=771&h=324](https://docs.google.com/drawings/d/1N-TnXQF19nwFYxkuWq_yUWSQ77IH147hZ0d5gDcgXr6/pub?w=771&h=324)

위를 보면 다 이해가 될것.

더 이상 얘기하면,,,,,, 쿠두 포스팅이 아니라 하둡 포스팅이 될것.. (이하 하둡포스팅으로 정리를 미룬다)

### 그렇다면 Kudu의 특징은?

- MapReduce, Spark 및 기타 하둡 컴포넌트와 통합됨
- OLAP 워크로드를 빠르게 처리
- 랜덤 액세스 뿐만 아니라, 인덱스(Primary Key)를 활용한 Sequential Access 또한 동시에 처리
- 정형 데이터 모델

.. 기타 내가 이해 못하는것은 적지 않았다.

### Kudu는 HBase와 비교해서 어떤 점이 좋나?

쿠두는 빠른 데이터에대한 빠른 분석이 필요할때 사용되었다.

- HBase가 row key를 제공하는 것과 비슷하게 Kudu에서는 primary 키를 제공해서 밀리초 수준의 랜덤 액세스가 가능
- 일반 DBMS처럼 PK가 있어서 대규모 순차 읽기에 매우 최적화 되어있다
- 테이블 생성시 각 칼럼마다 압축 방식과 인코딩 방식을 사용자가 직접 지정 가능
- 데이터 업데이트를 준 실시간으로 처리 가능
    - 데이터 예측 학습 진행시, kudu로 특정 값을 변경하고 재실행해서 시간 단위를 테스트별로 조정가능

이 다음에는 하둡과 아파치 스파크 정리를 해보는 것으로... (순서가 잘못된듯한 ㅎㅋㅎㅋ)

---

- **Reference**

[https://velog.io/@kimdukbae/MapReduce](https://velog.io/@kimdukbae/MapReduce)

[https://www.joinc.co.kr/w/man/12/hadoop/hbase/about](https://www.joinc.co.kr/w/man/12/hadoop/hbase/about)

[https://hgserver.tistory.com/30](https://hgserver.tistory.com/30)