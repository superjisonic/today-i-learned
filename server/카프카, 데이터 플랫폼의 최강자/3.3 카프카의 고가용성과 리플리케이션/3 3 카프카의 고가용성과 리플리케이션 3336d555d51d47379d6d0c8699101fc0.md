# 3.3 카프카의 고가용성과 리플리케이션

생성일: 2021년 3월 13일 오전 3:31

카프카의 리플리케이션 → 토픽을 이루는 각각의 `파티션`을 리플리케이션

# 1. 리플리케이션 팩터와 리더, 팔로워의 역할

리플리케이션 팩터 :  몇개의 파티션을 복제할지 설정

### 카프카 설정 파일에서 수정

```bash
vi /usr/local/kafka/config/server.properties
```

```bash
default.replication.factor = 1
```

- 기본 설정값은 1 로 되어있음 (설정파일에서 해당 항목이 없다면 기본값이 적용되어있는 것)
- 이 설정 값은 아무런 옵션을 주지 않고 토픽을 생성할 때 적용되는 값
    - 각 토픽별로 다른 리플리케이션 팩터 값을 설정 가능 (운영중에도 변경 가능)
- 클러스터 내 모든 브로커에 동일하게 설정해야함

### 설정 값이 반영된것 확인

```bash
cat /usr/local/kafka/logs/server.log
```

### 리플리케이션을 통한 높은 가용성 확인

- 토픽이 아니라 토픽을 이루는 각각의 `파티션`을 리플리케이션 하는것임

1. 리플리케이션이 되어있지 않은 경우

![3%203%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%E1%84%8B%E1%85%B4%20%E1%84%80%E1%85%A9%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%80%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%91%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%84%8F%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%203336d555d51d47379d6d0c8699101fc0/Untitled.png](3%203%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%E1%84%8B%E1%85%B4%20%E1%84%80%E1%85%A9%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%80%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%91%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%84%8F%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%203336d555d51d47379d6d0c8699101fc0/Untitled.png)

- 카프카 클러스터에 3대의 브로커가 구성되어있음
- 프로듀서 → peter 토픽으로 메시징
- peter토픽은 브로커1에만 위치함 (아직 리플리케이션이 구성되지 않았기 때문)

> 여기서 브로커3이 다운 → peter토픽은 브로커1에만 있으므로 영향X
*하지만 **브로커1**이 다운된다면??*

→ 프로듀서가 peter 토픽으로 보내는 메시지 요청을 처리할 수 없을 뿐만 아니라 프로듀서의 요청에 대한 응답도 주지 못함 : `대장애`

2. 리플리케이션이 되어있을 경우

```bash
default.replication.factor = 2
```

![3%203%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%E1%84%8B%E1%85%B4%20%E1%84%80%E1%85%A9%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%80%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%91%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%84%8F%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%203336d555d51d47379d6d0c8699101fc0/Untitled%201.png](3%203%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%E1%84%8B%E1%85%B4%20%E1%84%80%E1%85%A9%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%80%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%91%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%84%8F%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%203336d555d51d47379d6d0c8699101fc0/Untitled%201.png)

- peter 토픽은 브로커1과 브로커2에 위치함
    - 동일하게 리플리케이션을 서로 구분하는 법 (원본과 복제본)

    *→ 주키퍼 : 리더, 팔로워*

    → 래빗엠큐 : 마스터큐, 미러드큐

### 리더와 팔로워

- 리더 : 모든 읽기와 쓰기가 리더를 통해서만 일어남
- 팔로워 : 리더의 데이터를 그대로 리플리케이션만 하고 R/W에는 관여X

→ 두개 모두 저장된 데이터의 순서도 일치하고 동일한 오프셋과 메시지들을 가짐

> *토픽의 리더/팔로워가 각각 어느 브로커에 위치하는 지?*

- 주키퍼 3대 (주키퍼 클러스터의 서버가 3대)
- peter 토픽 : 파티션 1, 리플리케이션 팩터 2 옵션으로 생성한 경우

```bash
/usr/local/kafka/bin/kafka-topics.sh \
--zookeeper peter-zk001:2181, peter-zk002:2181, peter-zk003:2181/peter-kafka \
--topic peter --partitions 1 --replication-factor 2 --create
```

Created topic "peter". 라는 출력이 뜨면..

**[리더와 팔로워 확인]**

`--describe`라는 옵션 추가

```bash
/usr/local/kafka/bin/kafka-topics.sh \
--zookeeper peter-zk001:2181, peter-zk002:2181, peter-zk003:2181/peter-kafka \
--topic peter --describe
```

[출력]

Topic:peter   PartitionCount: 1    ReplicationFactor:2    Configs:

Topic: peter    Partition: 0    Leader: 1    Replicas:1,2    Isr:1,2

- Leader: 1은 peter토픽의 0번 파티션 `리더가 1번 브로커`에 있다는 의미
- Replicas:1,2는 peter 토픽이 리플리케이션 되고 있으며 → 브로커 1,2에 위치한다는 의미

    즉, Leader가 1번이므로 나머지 `2번이 팔로워`

### 장애시 리플리케이션이 구성되어 있다면

![3%203%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%E1%84%8B%E1%85%B4%20%E1%84%80%E1%85%A9%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%80%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%91%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%84%8F%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%203336d555d51d47379d6d0c8699101fc0/Untitled%202.png](3%203%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%E1%84%8B%E1%85%B4%20%E1%84%80%E1%85%A9%E1%84%80%E1%85%A1%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%80%E1%85%AA%20%E1%84%85%E1%85%B5%E1%84%91%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%84%8F%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%203336d555d51d47379d6d0c8699101fc0/Untitled%202.png)

- 브로커 1이 다운되면서 peter토픽의 리더도 다운되었다
- 브로커 2에 남아있는 peter토픽의 팔로워가 새로운 리더가 되어 프로듀서 요청에 응답

### 주의해야할 점

- 저장소 디스크 크기 주의
    - 토픽 사이즈가 100GB고, 리플리케이션 팩터가 2라면 브로커1,2 모두가 100GB를 사용
    - 카프카 클러스터 내 필요 저장소 크기 = 200GB
- 브로커의 리소스 사용량 증가
    - 브로커에서 비활성화된 토픽이 리플리케이션 잘 하고 있는지 모니터링하는 작업이 이뤄짐

토픽마다 데이터의 중요도에 따라 리플리케이션 팩터를 다르게 적용하여 운영하자

# 2. 리더와 팔로워 관리

리더 ← 팔로워 : 주기적으로 보면서 자신에게 없는 데이터를 가져옴

> 만약 팔로워가 리더로부터 데이터를 못갖고와서 정합성이 맞지 않을 경우?
⇒ ISR(In Sync Replica) 개념 도입

### ISR(In Sync Replica)

: 리플리케이션되고 있는 리플리케이션 그룹

- ISR에 속해 있는 구성원만이 리더 자격을 가질 수 있음
- ISR이라는 그룹을 통해 리플리케이션의 신뢰성을 높이고 있음

[ISR 축소 동작] : 리더 1, 팔로워2, ⇒ 리플리케이션 팩터 3 (리더와 팔로워 모두 ISR 구성원)

1. 프로듀서 - (A 메시지) → 토픽 리더
2. 리더 : 프로듀서의 요청을 받고 저장

    [`replica.lag.time.max.ms`](http://replica.lag.time.max.ms) 에 설정된 일정 주기동안 데이터 확인

    팔로워1 →  리더 : A 메시지 복제

    팔로워2 : 잘 동작 안함

3. 리더가 해당 팔로워의 이상 감지
4. 리더가 팔로워2 를 ISR 그룹에서 추방
5. ISR 그룹 구성원이 3→2 축소됨
6. 팔로워 1은 기존처럼 리더에게 새로운 메시지가 있으면 리더로부터 pull(=컨슈머가 메시지 가져가는 방법) 해서 메시지 저장